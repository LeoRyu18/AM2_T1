{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2-final"
    },
    "colab": {
      "name": "AM2_T1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoRyu18/AM2_T1/blob/main/AM2_T1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTLc5BpZrN83"
      },
      "source": [
        "# 1º Trabalho de Aprendizado de Máquina II - UFSCar 2020/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y39FXlO3rN8_"
      },
      "source": [
        "---\n",
        "\n",
        "**Trabalho referente a aplicação de métodos de Classificação multirrótulo em múltiplos datasets.**\n",
        "\n",
        "**Professor**\n",
        "> Diego Furtado Silva\n",
        "\n",
        "**Participantes:**\n",
        "\n",
        ">Leonardo Ryu Takaki -> lrtakaki@estudante.ufscar.br\n",
        "\n",
        ">Augusto Rozendo Mends -> augustorm@estudante.ufscar.br\n",
        "\n",
        ">Fernando Sassi Nunes -> fernando.nunes@estudante.ufscar.br"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpa4HFoHrN9A"
      },
      "source": [
        "---\n",
        "\n",
        "- Algoritmos\n",
        " - MLkNN\n",
        " - Relevância binária (BR)\n",
        " - Label Powerset (LP)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Base de dados\n",
        " - Emotions\n",
        " - FoodTrucks\n",
        " - Birds\n",
        " - Scene\n",
        " -\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNlv1E5GtQPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6718998-6bc4-4b8c-f68c-b7b63c1ee36e"
      },
      "source": [
        "!pip install scikit-multilearn"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EWIkp5orN9B"
      },
      "source": [
        "# importações\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "from skmultilearn.adapt import MLkNN\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "simplefilter(action='ignore', category=FitFailedWarning)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aLJzzEPrN9C"
      },
      "source": [
        "---\n",
        "## MLkNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aqS7cEprN9D"
      },
      "source": [
        "* Derivado de dois algoritmos monorrótulos: KNN e Naive Bayes;\n",
        "\n",
        "* 1) KNN é aplicado, identificando os k vizinhos mais próximos do novo objeto na base de treinamento.\n",
        "\n",
        "* 2) A Fórmula de Bayes é empregada para cada rótulo de classe lj presente no labelset destes k vizinhos para estimar se t deverá ou não ser rotulado com lj.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYqlLhatrN9E"
      },
      "source": [
        "def MLkNN_simples(X_train, y_train, X_test, y_test):\n",
        "    classifier = MLkNN(k=3)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    predictions = classifier.predict(X_test)\n",
        "    print(\"Hamming Loss =\", metrics.hamming_loss(y_test, predictions))\n",
        "    print(\"Acurácia =\", metrics.accuracy_score(y_test, predictions))\n",
        "    print(\"Precisão =\", metrics.precision_score(y_test, predictions, average='micro'))\n",
        "    print(\"Revocação =\", metrics.recall_score(y_test, predictions, average='micro'))\n",
        "\n",
        "def MLkNN_GS(X_train, y_train, X_test, y_test):\n",
        "    parameters = {'k': range(1,10), 's': [0.1, 0.3, 0.5, 0.7, 1.0]}\n",
        "\n",
        "    model = GridSearchCV(MLkNN(), parameters, scoring='accuracy')\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Melhores parâmetros =\", model.best_params_,\"\\nMelhor score =\", model.best_score_)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    \n",
        "    Hamming_Loss = metrics.hamming_loss(y_test, predictions)\n",
        "    Acuracia = metrics.accuracy_score(y_test, predictions)\n",
        "    Precisão = metrics.precision_score(y_test, predictions, average='micro')\n",
        "    Revocação = metrics.recall_score(y_test, predictions, average='micro')\n",
        "\n",
        "    print(\"Hamming Loss =\", Hamming_Loss)\n",
        "    print(\"Acurácia =\", Acuracia)\n",
        "    print(\"Precisão =\", Precisão)\n",
        "    print(\"Revocação =\", Revocação)\n",
        "    return Hamming_Loss, Acuracia, Precisão, Revocação"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9XAZXx4rN9F"
      },
      "source": [
        "---\n",
        "## Relevância binária (BR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0olbpyfFrN9G"
      },
      "source": [
        "* Um classificador binário de rótulo único é treinado para cada classe.\n",
        "\n",
        "* Cada classificador prevê a associação ou a não associação de uma classe para um objeto. A união de todas as classes previstas será a saída.\n",
        "\n",
        "* Considera independência entre os rótulos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWM0t2TbrN9H"
      },
      "source": [
        "def BR_simples(X_train, y_train, X_test, y_test):\n",
        "    classifier = BinaryRelevance( classifier = SVC(), require_dense = [False, True])\n",
        "    classifier.fit(X_train, y_train)\n",
        "    predictions = classifier.predict(X_test)\n",
        "    print(\"Hamming Loss =\", metrics.hamming_loss(y_test, predictions))\n",
        "    print(\"Acurácia =\", metrics.accuracy_score(y_test, predictions))\n",
        "    print(\"Precisão =\", metrics.precision_score(y_test, predictions, average='micro'))\n",
        "    print(\"Revocação =\", metrics.recall_score(y_test, predictions, average='micro'))\n",
        "\n",
        "def BR_GS(X_train, y_train, X_test, y_test):\n",
        "    parameters = [{'classifier': [MultinomialNB()],\n",
        "                   'classifier__alpha': [0.5, 0.7, 1.0],\n",
        "                  },\n",
        "                  {'classifier': [SVC()],\n",
        "                   'classifier__kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "                  },\n",
        "                  {'classifier':[GaussianNB()],\n",
        "                   'classifier__var_smoothing':[0.00000001, 0.000000001, 0.00000001],\n",
        "                  },\n",
        "                  {'classifier': [RandomForestClassifier()],\n",
        "                   'classifier__criterion': ['gini', 'entropy'],\n",
        "                   'classifier__n_estimators': [10, 20, 50],\n",
        "                  },\n",
        "                 ]\n",
        "    model = GridSearchCV(BinaryRelevance(), parameters, scoring='accuracy')\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Melhores parâmetros =\", model.best_params_,\"\\nMelhor score =\", model.best_score_)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    Hamming_Loss = metrics.hamming_loss(y_test, predictions)\n",
        "    Acuracia = metrics.accuracy_score(y_test, predictions)\n",
        "    Precisão = metrics.precision_score(y_test, predictions, average='micro')\n",
        "    Revocação = metrics.recall_score(y_test, predictions, average='micro')\n",
        "\n",
        "    print(\"Hamming Loss =\", Hamming_Loss)\n",
        "    print(\"Acurácia =\", Acuracia)\n",
        "    print(\"Precisão =\", Precisão)\n",
        "    print(\"Revocação =\", Revocação)\n",
        "    return Hamming_Loss, Acuracia, Precisão, Revocação"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwP5bk7GrN9H"
      },
      "source": [
        "---\n",
        "## LP (Label Powerset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E4839_ArN9I"
      },
      "source": [
        "* Essa abordagem leva em consideração as possíveis correlações entre os rótulos de classe.\n",
        "\n",
        "* Considera cada combinação das classes como um único rótulo.\n",
        "\n",
        "* Tem uma alta complexidade computacional, tendo em vista que as combinações distintas de rótulos cresce exponencialmente quando o número de classes aumenta, tornando o problema difícil.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3VSkVCgrN9J"
      },
      "source": [
        "def LP_simples(X_train, y_train, X_test, y_test):\n",
        "    classifier = ClassifierChain(classifier = RandomForestClassifier(n_estimators=100), require_dense = [False, True])\n",
        "    classifier.fit(X_train, y_train)\n",
        "    predictions = classifier.predict(X_test)\n",
        "    print(\"Hamming Loss =\", metrics.hamming_loss(y_test, predictions))\n",
        "    print(\"Acurácia =\", metrics.accuracy_score(y_test, predictions))\n",
        "    print(\"Precisão =\", metrics.precision_score(y_test, predictions, average='micro'))\n",
        "    print(\"Revocação =\", metrics.recall_score(y_test, predictions, average='micro'))\n",
        "\n",
        "\n",
        "def LP_GS(X_train, y_train, X_test, y_test):\n",
        "    parameters = [{'classifier': [MultinomialNB()],\n",
        "                   'classifier__alpha': [0.5, 0.7, 1.0],\n",
        "                  },\n",
        "                  {'classifier': [RandomForestClassifier()],\n",
        "                   'classifier__criterion': ['gini', 'entropy'],\n",
        "                   'classifier__n_estimators': [10, 20, 50],\n",
        "                  },\n",
        "                  {'classifier': [SVC()],\n",
        "                   'classifier__kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "                  },\n",
        "                  {'classifier':[GaussianNB()],\n",
        "                   'classifier__var_smoothing':[0.00000001, 0.000000001, 0.00000001],\n",
        "                  },\n",
        "                 ]\n",
        "    model = GridSearchCV(LabelPowerset(), parameters, scoring='accuracy')\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Melhores parâmetros =\", model.best_params_,\"\\nMelhor score =\", model.best_score_)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    Hamming_Loss = metrics.hamming_loss(y_test, predictions)\n",
        "    Acuracia = metrics.accuracy_score(y_test, predictions)\n",
        "    Precisão = metrics.precision_score(y_test, predictions, average='micro')\n",
        "    Revocação = metrics.recall_score(y_test, predictions, average='micro')\n",
        "\n",
        "    print(\"Hamming Loss =\", Hamming_Loss)\n",
        "    print(\"Acurácia =\", Acuracia)\n",
        "    print(\"Precisão =\", Precisão)\n",
        "    print(\"Revocação =\", Revocação)\n",
        "    return Hamming_Loss, Acuracia, Precisão, Revocação\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty09VyRLrN9K"
      },
      "source": [
        "---\n",
        "\n",
        "# Datasets + aplicação dos algoritmos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho4-S0CIrN9K"
      },
      "source": [
        "## Emotions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNDcirXLwkxI"
      },
      "source": [
        "Dataset obtido em http://mulan.sourceforge.net/datasets-mlc.html\n",
        "\n",
        "Os humanos, por natureza, são afetados emocionalmente pela música. À medida que os bancos de dados de música aumentam de tamanho, a recuperação da música pela emoção se torna uma\n",
        "tarefa importante para várias aplicações, como seleção de música em dispositivos móveis, sistemas de recomendação de música, programas de TV, rádio e musicoterapia.\n",
        "A música pode evocar mais de uma emoção diferente ao mesmo tempo. Desta forma, queremos recuperar uma peça musical baseada em qualquer uma das emoções. A classificação e regressão de rótulo único não podem modelar essa multiplicidade, portanto, utilizamos alguns algoritmos de classificação multirrótulo.\n",
        "\n",
        "Os features extraídos se enquadram em duas categorias: rítmica e timbre.\n",
        "\n",
        "* Features Rítmicos:\n",
        "As características rítmicas foram derivadas da extração das alterações periódicas de um histograma de batida. Um algoritmo que identifica picos foi utilizado e os dois picos mais altos foram selecionados para calcular as amplitudes,\n",
        "seus BPMs (batidas por minuto) e a proporção de alta BPM e baixa BPM. Além disso, 3 recursos foram calculados somando os bins do histograma entre 40-90, 90-140 e 140-\n",
        "250 BPMs respectivamente. Desta forma, temos \n",
        "8 features rítmicos.\n",
        "\n",
        "* Features de timbre:\n",
        "Coeficientes mel-cepstrais (MFCCs) são usados ​​para\n",
        "reconhecimento de fala e modelagem musical. Para derivar features MFCCs, o sinal foi dividido em frames e o espectro de amplitude foi calculado para cada frame. Em seguida, seu logaritmo foi obtido e convertido para a escala de Mel. Finalmente, a\n",
        "transformada discreta de cosseno foi aplicada. No Dataset selecionado, foram utilizados os primeiros 13 MFCCs.\n",
        "Outro conjunto de 3 features relacionados a texturas de timbre foram extraídos da Transformada de Fourier de Curto Termo (FFT): Centróide espectral, rolloff espectral e fluxo espectral.\n",
        "Para cada um dos 16 recursos mencionados acima (13 MFCCs e 3 FFT) foram calculadas a média, o desvio padrão (std),\n",
        "desvio padrão médio (mean std) e desvio padrão\n",
        "de desvio padrão (std std) em todos os frames. Isso levou a um total de 64 recursos de timbre.\n",
        "\n",
        "Temos 6 classes: amazed-surprised, happy-pleased, relaxing-calm, quiet-still, sad-lonely, angry-fearful. Os exemplos foram classificados por três especialistas do sexo masculino de 20, 25 e 30 anos. Apenas as músicas com rotulagem completamente idêntica entre os especialistas foram mantidas para experimentação subsequente. Esse processo levou a um conjunto de dados com 593 músicas classificados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVGSVDIrN9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "5293cc32-1971-49b1-f174-8840714ca5ad"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/LeoRyu18/AM2_T1/main/emotions.csv?token=AHW73BL6QJI2YHTKQFMQHTDAKVXNU\")\n",
        "df.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_10</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_11</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_12</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Rolloff</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Flux</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_4</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_5</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_6</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_7</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_10</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_11</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_12</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_8</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_10</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_11</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_12</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_10</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_11</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_12</th>\n",
              "      <th>BH_LowPeakAmp</th>\n",
              "      <th>BH_LowPeakBPM</th>\n",
              "      <th>BH_HighPeakAmp</th>\n",
              "      <th>BH_HighPeakBPM</th>\n",
              "      <th>BH_HighLowRatio</th>\n",
              "      <th>BHSUM1</th>\n",
              "      <th>BHSUM2</th>\n",
              "      <th>BHSUM3</th>\n",
              "      <th>amazed-suprised</th>\n",
              "      <th>happy-pleased</th>\n",
              "      <th>relaxing-calm</th>\n",
              "      <th>quiet-still</th>\n",
              "      <th>sad-lonely</th>\n",
              "      <th>angry-aggresive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.034741</td>\n",
              "      <td>0.089665</td>\n",
              "      <td>0.091225</td>\n",
              "      <td>-73.302422</td>\n",
              "      <td>6.215179</td>\n",
              "      <td>0.615074</td>\n",
              "      <td>2.037160</td>\n",
              "      <td>0.804065</td>\n",
              "      <td>1.301409</td>\n",
              "      <td>0.558576</td>\n",
              "      <td>0.672063</td>\n",
              "      <td>0.783788</td>\n",
              "      <td>0.766640</td>\n",
              "      <td>0.458712</td>\n",
              "      <td>0.530384</td>\n",
              "      <td>0.812429</td>\n",
              "      <td>0.028851</td>\n",
              "      <td>0.129039</td>\n",
              "      <td>0.039614</td>\n",
              "      <td>5.762173</td>\n",
              "      <td>1.636819</td>\n",
              "      <td>1.170034</td>\n",
              "      <td>1.051511</td>\n",
              "      <td>0.764163</td>\n",
              "      <td>0.642705</td>\n",
              "      <td>0.617868</td>\n",
              "      <td>0.510265</td>\n",
              "      <td>0.566213</td>\n",
              "      <td>0.509149</td>\n",
              "      <td>0.477275</td>\n",
              "      <td>0.505073</td>\n",
              "      <td>0.463535</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.050591</td>\n",
              "      <td>0.009025</td>\n",
              "      <td>8.156257</td>\n",
              "      <td>1.077167</td>\n",
              "      <td>0.624711</td>\n",
              "      <td>0.810244</td>\n",
              "      <td>0.399568</td>\n",
              "      <td>0.279947</td>\n",
              "      <td>0.314215</td>\n",
              "      <td>0.231439</td>\n",
              "      <td>0.345401</td>\n",
              "      <td>0.285389</td>\n",
              "      <td>0.210613</td>\n",
              "      <td>0.321896</td>\n",
              "      <td>0.290551</td>\n",
              "      <td>0.022774</td>\n",
              "      <td>0.095801</td>\n",
              "      <td>0.015057</td>\n",
              "      <td>4.748694</td>\n",
              "      <td>0.536378</td>\n",
              "      <td>0.296306</td>\n",
              "      <td>0.273210</td>\n",
              "      <td>0.175800</td>\n",
              "      <td>0.105508</td>\n",
              "      <td>0.168246</td>\n",
              "      <td>0.115849</td>\n",
              "      <td>0.136020</td>\n",
              "      <td>0.110514</td>\n",
              "      <td>0.100517</td>\n",
              "      <td>0.118630</td>\n",
              "      <td>0.094923</td>\n",
              "      <td>0.051035</td>\n",
              "      <td>68</td>\n",
              "      <td>0.014937</td>\n",
              "      <td>136</td>\n",
              "      <td>2</td>\n",
              "      <td>0.245457</td>\n",
              "      <td>0.105065</td>\n",
              "      <td>0.405399</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.081374</td>\n",
              "      <td>0.272747</td>\n",
              "      <td>0.085733</td>\n",
              "      <td>-62.584437</td>\n",
              "      <td>3.183163</td>\n",
              "      <td>-0.218145</td>\n",
              "      <td>0.163038</td>\n",
              "      <td>0.620251</td>\n",
              "      <td>0.458514</td>\n",
              "      <td>0.041426</td>\n",
              "      <td>0.308287</td>\n",
              "      <td>0.538152</td>\n",
              "      <td>0.594871</td>\n",
              "      <td>0.734332</td>\n",
              "      <td>0.415489</td>\n",
              "      <td>0.761508</td>\n",
              "      <td>0.066288</td>\n",
              "      <td>0.262370</td>\n",
              "      <td>0.034438</td>\n",
              "      <td>3.480874</td>\n",
              "      <td>1.596532</td>\n",
              "      <td>0.943803</td>\n",
              "      <td>0.804444</td>\n",
              "      <td>0.511229</td>\n",
              "      <td>0.498670</td>\n",
              "      <td>0.523039</td>\n",
              "      <td>0.480916</td>\n",
              "      <td>0.488657</td>\n",
              "      <td>0.483166</td>\n",
              "      <td>0.445187</td>\n",
              "      <td>0.415994</td>\n",
              "      <td>0.405593</td>\n",
              "      <td>0.013621</td>\n",
              "      <td>0.073041</td>\n",
              "      <td>0.010094</td>\n",
              "      <td>1.243981</td>\n",
              "      <td>0.829790</td>\n",
              "      <td>0.252972</td>\n",
              "      <td>0.347831</td>\n",
              "      <td>0.205087</td>\n",
              "      <td>0.168601</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.144080</td>\n",
              "      <td>0.178703</td>\n",
              "      <td>0.146937</td>\n",
              "      <td>0.125580</td>\n",
              "      <td>0.128202</td>\n",
              "      <td>0.107007</td>\n",
              "      <td>0.020028</td>\n",
              "      <td>0.066940</td>\n",
              "      <td>0.029483</td>\n",
              "      <td>3.963534</td>\n",
              "      <td>0.382360</td>\n",
              "      <td>0.168389</td>\n",
              "      <td>0.117525</td>\n",
              "      <td>0.098341</td>\n",
              "      <td>0.087046</td>\n",
              "      <td>0.057991</td>\n",
              "      <td>0.059393</td>\n",
              "      <td>0.059457</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.067684</td>\n",
              "      <td>0.070075</td>\n",
              "      <td>0.041565</td>\n",
              "      <td>0.295031</td>\n",
              "      <td>70</td>\n",
              "      <td>0.276366</td>\n",
              "      <td>140</td>\n",
              "      <td>2</td>\n",
              "      <td>0.343547</td>\n",
              "      <td>0.276366</td>\n",
              "      <td>0.710924</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.110545</td>\n",
              "      <td>0.273567</td>\n",
              "      <td>0.084410</td>\n",
              "      <td>-65.235325</td>\n",
              "      <td>2.794964</td>\n",
              "      <td>0.639047</td>\n",
              "      <td>1.281297</td>\n",
              "      <td>0.757896</td>\n",
              "      <td>0.489412</td>\n",
              "      <td>0.627636</td>\n",
              "      <td>0.469322</td>\n",
              "      <td>0.644336</td>\n",
              "      <td>0.441556</td>\n",
              "      <td>0.335964</td>\n",
              "      <td>0.290713</td>\n",
              "      <td>0.158538</td>\n",
              "      <td>0.082743</td>\n",
              "      <td>0.215373</td>\n",
              "      <td>0.035970</td>\n",
              "      <td>4.834742</td>\n",
              "      <td>1.213443</td>\n",
              "      <td>0.864034</td>\n",
              "      <td>0.909222</td>\n",
              "      <td>0.780572</td>\n",
              "      <td>0.550833</td>\n",
              "      <td>0.639740</td>\n",
              "      <td>0.573309</td>\n",
              "      <td>0.526312</td>\n",
              "      <td>0.562622</td>\n",
              "      <td>0.538407</td>\n",
              "      <td>0.492292</td>\n",
              "      <td>0.455562</td>\n",
              "      <td>0.029112</td>\n",
              "      <td>0.070433</td>\n",
              "      <td>0.008525</td>\n",
              "      <td>2.759906</td>\n",
              "      <td>0.592634</td>\n",
              "      <td>0.761852</td>\n",
              "      <td>0.568740</td>\n",
              "      <td>0.589827</td>\n",
              "      <td>0.281181</td>\n",
              "      <td>0.437752</td>\n",
              "      <td>0.479889</td>\n",
              "      <td>0.227320</td>\n",
              "      <td>0.296224</td>\n",
              "      <td>0.273855</td>\n",
              "      <td>0.191804</td>\n",
              "      <td>0.198025</td>\n",
              "      <td>0.038119</td>\n",
              "      <td>0.065427</td>\n",
              "      <td>0.029622</td>\n",
              "      <td>3.371796</td>\n",
              "      <td>0.430373</td>\n",
              "      <td>0.172862</td>\n",
              "      <td>0.177523</td>\n",
              "      <td>0.184333</td>\n",
              "      <td>0.095718</td>\n",
              "      <td>0.139323</td>\n",
              "      <td>0.109279</td>\n",
              "      <td>0.090650</td>\n",
              "      <td>0.117886</td>\n",
              "      <td>0.100852</td>\n",
              "      <td>0.079917</td>\n",
              "      <td>0.085821</td>\n",
              "      <td>0.161574</td>\n",
              "      <td>61</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>183</td>\n",
              "      <td>3</td>\n",
              "      <td>0.188693</td>\n",
              "      <td>0.045941</td>\n",
              "      <td>0.457372</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.042481</td>\n",
              "      <td>0.199281</td>\n",
              "      <td>0.093447</td>\n",
              "      <td>-80.305152</td>\n",
              "      <td>5.824409</td>\n",
              "      <td>0.648848</td>\n",
              "      <td>1.754870</td>\n",
              "      <td>1.495532</td>\n",
              "      <td>0.739909</td>\n",
              "      <td>0.809644</td>\n",
              "      <td>0.460945</td>\n",
              "      <td>0.409566</td>\n",
              "      <td>0.680122</td>\n",
              "      <td>0.590405</td>\n",
              "      <td>0.481380</td>\n",
              "      <td>0.621956</td>\n",
              "      <td>0.049939</td>\n",
              "      <td>0.281616</td>\n",
              "      <td>0.044727</td>\n",
              "      <td>6.719538</td>\n",
              "      <td>1.377811</td>\n",
              "      <td>1.265771</td>\n",
              "      <td>0.986178</td>\n",
              "      <td>0.710955</td>\n",
              "      <td>0.706904</td>\n",
              "      <td>0.710147</td>\n",
              "      <td>0.688825</td>\n",
              "      <td>0.699573</td>\n",
              "      <td>0.577976</td>\n",
              "      <td>0.533882</td>\n",
              "      <td>0.501818</td>\n",
              "      <td>0.495368</td>\n",
              "      <td>0.020749</td>\n",
              "      <td>0.106318</td>\n",
              "      <td>0.009108</td>\n",
              "      <td>3.992357</td>\n",
              "      <td>0.656429</td>\n",
              "      <td>0.927692</td>\n",
              "      <td>0.569916</td>\n",
              "      <td>0.378919</td>\n",
              "      <td>0.530714</td>\n",
              "      <td>0.317807</td>\n",
              "      <td>0.308447</td>\n",
              "      <td>0.324934</td>\n",
              "      <td>0.263444</td>\n",
              "      <td>0.359477</td>\n",
              "      <td>0.274257</td>\n",
              "      <td>0.233287</td>\n",
              "      <td>0.032678</td>\n",
              "      <td>0.119480</td>\n",
              "      <td>0.028707</td>\n",
              "      <td>4.125111</td>\n",
              "      <td>0.461304</td>\n",
              "      <td>0.280751</td>\n",
              "      <td>0.246108</td>\n",
              "      <td>0.142805</td>\n",
              "      <td>0.183657</td>\n",
              "      <td>0.124399</td>\n",
              "      <td>0.155513</td>\n",
              "      <td>0.167114</td>\n",
              "      <td>0.113774</td>\n",
              "      <td>0.112815</td>\n",
              "      <td>0.129145</td>\n",
              "      <td>0.122330</td>\n",
              "      <td>0.043012</td>\n",
              "      <td>66</td>\n",
              "      <td>0.206562</td>\n",
              "      <td>132</td>\n",
              "      <td>2</td>\n",
              "      <td>0.102839</td>\n",
              "      <td>0.241934</td>\n",
              "      <td>0.351009</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.074550</td>\n",
              "      <td>0.140880</td>\n",
              "      <td>0.079789</td>\n",
              "      <td>-93.697749</td>\n",
              "      <td>5.543229</td>\n",
              "      <td>1.064262</td>\n",
              "      <td>0.899152</td>\n",
              "      <td>0.890336</td>\n",
              "      <td>0.702328</td>\n",
              "      <td>0.490685</td>\n",
              "      <td>0.796904</td>\n",
              "      <td>0.745373</td>\n",
              "      <td>0.911234</td>\n",
              "      <td>0.594429</td>\n",
              "      <td>0.454186</td>\n",
              "      <td>0.384836</td>\n",
              "      <td>0.035751</td>\n",
              "      <td>0.085592</td>\n",
              "      <td>0.029413</td>\n",
              "      <td>4.755293</td>\n",
              "      <td>1.116290</td>\n",
              "      <td>0.926772</td>\n",
              "      <td>0.634988</td>\n",
              "      <td>0.639660</td>\n",
              "      <td>0.552653</td>\n",
              "      <td>0.527708</td>\n",
              "      <td>0.584705</td>\n",
              "      <td>0.696173</td>\n",
              "      <td>0.648611</td>\n",
              "      <td>0.689096</td>\n",
              "      <td>0.643595</td>\n",
              "      <td>0.578063</td>\n",
              "      <td>0.047014</td>\n",
              "      <td>0.136984</td>\n",
              "      <td>0.010356</td>\n",
              "      <td>7.713140</td>\n",
              "      <td>1.592642</td>\n",
              "      <td>1.027190</td>\n",
              "      <td>0.591399</td>\n",
              "      <td>0.565654</td>\n",
              "      <td>0.524420</td>\n",
              "      <td>0.554501</td>\n",
              "      <td>0.606200</td>\n",
              "      <td>0.616760</td>\n",
              "      <td>0.596926</td>\n",
              "      <td>0.524291</td>\n",
              "      <td>0.637971</td>\n",
              "      <td>0.637960</td>\n",
              "      <td>0.036151</td>\n",
              "      <td>0.087741</td>\n",
              "      <td>0.030180</td>\n",
              "      <td>5.085385</td>\n",
              "      <td>0.551937</td>\n",
              "      <td>0.257562</td>\n",
              "      <td>0.159950</td>\n",
              "      <td>0.175855</td>\n",
              "      <td>0.150907</td>\n",
              "      <td>0.142092</td>\n",
              "      <td>0.222804</td>\n",
              "      <td>0.329188</td>\n",
              "      <td>0.251668</td>\n",
              "      <td>0.265049</td>\n",
              "      <td>0.284196</td>\n",
              "      <td>0.189988</td>\n",
              "      <td>0.029308</td>\n",
              "      <td>100</td>\n",
              "      <td>0.144039</td>\n",
              "      <td>200</td>\n",
              "      <td>2</td>\n",
              "      <td>0.195196</td>\n",
              "      <td>0.310801</td>\n",
              "      <td>0.683817</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  Mean_Acc1298_Mean_Mem40_Centroid  ...  sad-lonely  angry-aggresive\n",
              "0   1                          0.034741  ...           0                0\n",
              "1   2                          0.081374  ...           0                1\n",
              "2   3                          0.110545  ...           0                1\n",
              "3   4                          0.042481  ...           0                0\n",
              "4   5                          0.074550  ...           0                0\n",
              "\n",
              "[5 rows x 79 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC2Kq7HdrN9M"
      },
      "source": [
        "# Pré-processamento \n",
        "\n",
        "df = df.drop(columns=['id'])\n",
        "\n",
        "class_pos = [\"amazed-suprised\", \"happy-pleased\", \"relaxing-calm\", \"quiet-still\", \"sad-lonely\", \"angry-aggresive\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(class_pos, axis = 1), df[class_pos],  test_size = 0.3, random_state = 1)\n",
        "\n",
        "# Normalização\n",
        "X_train = preprocessing.MinMaxScaler().fit_transform(X_train)\n",
        "X_test = preprocessing.MinMaxScaler().fit_transform(X_test)\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXEXAHd0rN9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b858bb01-8e39-4283-f0a9-c9ad40a3e0da"
      },
      "source": [
        "# MLkNN\n",
        "print(\"MLkNN simples:\")\n",
        "MLkNN_simples(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nMLkNN com Grid Search:\")\n",
        "\n",
        "MLkNN_Hamming_Loss_Emotions , MLkNN_Acuracia_Emotions, MLkNN_Precisão_Emotions , MLkNN_Revocação_Emotions = MLkNN_GS(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLkNN simples:\n",
            "Hamming Loss = 0.2144194756554307\n",
            "Acurácia = 0.29213483146067415\n",
            "Precisão = 0.6778115501519757\n",
            "Revocação = 0.6445086705202312\n",
            "\n",
            "MLkNN com Grid Search:\n",
            "Melhores parâmetros = {'k': 3, 's': 0.1} \n",
            "Melhor score = 0.28674698795180725\n",
            "Hamming Loss = 0.2144194756554307\n",
            "Acurácia = 0.29213483146067415\n",
            "Precisão = 0.6778115501519757\n",
            "Revocação = 0.6445086705202312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP39Hg4Pq0DM",
        "outputId": "34e45323-60cf-4b8f-dea6-c6d450fa21f9"
      },
      "source": [
        "MLkNN_Hamming_Loss_Emotions"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2144194756554307"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib5i08RHrN9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75b2df8-f0a1-4638-d4ca-a5c19d45e831"
      },
      "source": [
        "# BR\n",
        "print(\"BR simples:\")\n",
        "BR_simples(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nBR com Grid Search:\")\n",
        "BR_Hamming_Loss_Emotions , BR_Acuracia_Emotions, BR_Precisão_Emotions , BR_Revocação_Emotions = BR_GS(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BR simples:\n",
            "Hamming Loss = 0.18726591760299627\n",
            "Acurácia = 0.2640449438202247\n",
            "Precisão = 0.8173913043478261\n",
            "Revocação = 0.5433526011560693\n",
            "\n",
            "BR com Grid Search:\n",
            "Melhores parâmetros = {'classifier': SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False), 'classifier__kernel': 'rbf'} \n",
            "Melhor score = 0.30120481927710846\n",
            "Hamming Loss = 0.18726591760299627\n",
            "Acurácia = 0.2640449438202247\n",
            "Precisão = 0.8173913043478261\n",
            "Revocação = 0.5433526011560693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dYsXQ8srN9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e206fb-eef7-4c2a-8592-55df9fceaf39"
      },
      "source": [
        "# LP\n",
        "print(\"LP simples:\")\n",
        "LP_simples(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nLP com Grid Search:\")\n",
        "LP_Hamming_Loss_Emotions , LP_Acuracia_Emotions, LP_Precisão_Emotions , LP_Revocação_Emotions =LP_GS(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LP simples:\n",
            "Hamming Loss = 0.18726591760299627\n",
            "Acurácia = 0.30337078651685395\n",
            "Precisão = 0.7786259541984732\n",
            "Revocação = 0.5895953757225434\n",
            "\n",
            "LP com Grid Search:\n",
            "Melhores parâmetros = {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='entropy', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False), 'classifier__criterion': 'entropy', 'classifier__n_estimators': 50} \n",
            "Melhor score = 0.3253012048192771\n",
            "Hamming Loss = 0.20786516853932585\n",
            "Acurácia = 0.34831460674157305\n",
            "Precisão = 0.6823529411764706\n",
            "Revocação = 0.6705202312138728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA2TlNw4rN9N"
      },
      "source": [
        "\n",
        "---\n",
        "## FoodTrucks\n",
        "\n",
        "Este dataset (disponível em https://www.uco.es/kdis/mllresources/) contém resultados de uma pesquisa conduzida em Natal, que visava determinar preferência por certos tipos de food trucks dependendo de fatores diversos. São 407 exemplos no total.\n",
        "\n",
        "As features são:\n",
        "\n",
        "* frequência(numérico): com que frequência uma pessoa come fora, de 1(raramente) a 5(diariamente ou quase diariamente)\n",
        "\n",
        "* tempo (categórico): em que parte do dia costuma comer fora. Os possíveis valores são: manhã,almoço, tarde, happy hour e janta\n",
        "\n",
        "* gasto (numérico): quanto um dado indivíduo costuma gastar, em reais. Os valores determinam os limites dos intervalos: 15,20,30,40,50.\n",
        "\n",
        "* motivação (categórico): o que leva um indivíduo a escolher determinado foodtruck: propaganda, acaso, amigos, redes sociais, internet\n",
        "\n",
        "* gosto, higiene, menu, apresentação, atendimento, ingredientes, lugar disponível,para viagem, variação de escolha, encontros, horários: quanto um dado indivíduo valoriza determinadas qualidades de um foodtruck, numa escala de 1 a 5.\n",
        "\n",
        "* gênero(categórico)\n",
        "\n",
        "* escolaridade(numérico): de sem estudo a phd numa escala de 1 a 5. Vale notar que alunos em processo de graduação são representados por 1.5\n",
        "\n",
        "* renda média(numérico): em termos de salários mínimos, de 1 a 5, sendo 1 menos de 2 e 5 mais de 20\n",
        "\n",
        "* trabalha (binário)\n",
        "\n",
        "* estado civil (categórico)\n",
        "\n",
        "* idade (numérico): agrupados em intervalos, sendo o primeiro <19, os subsequentes de 5 em 5, terminando em um intervalo 50+. Assume os valores de 1 a 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjnTXaF_rN9O"
      },
      "source": [
        "df_food = pd.read_csv(\"https://raw.githubusercontent.com/LeoRyu18/AM2_T1/main/food.csv?token=AHW73BLI3WHUSEIWBNJPWKLAKVW3K\")\n",
        "df_food.head()\n",
        "df_food_test = df_food.drop(['motivation', 'expenses', 'taste', 'variation', 'ingredients'], axis = 1)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVZhP4DgrN9P"
      },
      "source": [
        "labels = ['italian_food','brazilian_food','mexican_food','chinese_food','japanese_food','arabic_food','snacks','healthy_food','fitness_food','sweets_desserts','gourmet','street_food']\n",
        "\n",
        "y = df_food[labels]\n",
        "X = df_food.drop(labels,axis=1)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX0BtyjtrN9P"
      },
      "source": [
        "#encoding de features categóricos\n",
        "X.replace({'dawn':1,'lunch':2,'afternoon':3,'happy_hour':4,'dinner':5\n",
        ",'ads':1,'by_chance':2,'friend':3,'social_network':4,'web':5,\n",
        "'F':1,'M':2,\n",
        "'divorced':1,'married':2,'single':3\n",
        "}, inplace=True)\n",
        "\n",
        "\n",
        "#Separação treino-teste\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42) \n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# o único feature que já não vem normalizado numa escala de 1 a 5 é a renda. Não sei se isso é um oversight ou intencional por parte do estudo mas eu decidi normalizar\n",
        "\n",
        "X_train = preprocessing.MinMaxScaler().fit_transform(X_train)\n",
        "X_test = preprocessing.MinMaxScaler().fit_transform(X_test)\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9jGhru_rN9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d6af02-ba42-40a4-9f9d-99552c7c0a26"
      },
      "source": [
        "print(\"BR simples:\")\n",
        "BR_simples(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nBR com Grid Search:\")\n",
        "BR_Hamming_Loss_FoodTrucks , BR_Acuracia_FoodTrucks, BR_Precisão_FoodTrucks , BR_Revocação_FoodTrucks = BR_GS(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BR simples:\n",
            "Hamming Loss = 0.16584967320261437\n",
            "Acurácia = 0.24509803921568626\n",
            "Precisão = 0.6608695652173913\n",
            "Revocação = 0.31666666666666665\n",
            "\n",
            "BR com Grid Search:\n",
            "Melhores parâmetros = {'classifier': MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True), 'classifier__alpha': 0.5} \n",
            "Melhor score = 0.28852459016393445\n",
            "Hamming Loss = 0.16666666666666666\n",
            "Acurácia = 0.2647058823529412\n",
            "Precisão = 0.6764705882352942\n",
            "Revocação = 0.2875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a39QdZ-orN9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1118e7aa-3925-44af-a7c0-aa787e978e0d"
      },
      "source": [
        "# MLkNN\n",
        "print(\"MLkNN simples:\")\n",
        "MLkNN_simples(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nMLkNN com Grid Search:\")\n",
        "MLkNN_Hamming_Loss_FoodTrucks , MLkNN_Acuracia_FoodTrucks, MLkNN_Precisão_FoodTrucks , MLkNN_Revocação_FoodTrucks = MLkNN_GS(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLkNN simples:\n",
            "Hamming Loss = 0.18137254901960784\n",
            "Acurácia = 0.16666666666666666\n",
            "Precisão = 0.5555555555555556\n",
            "Revocação = 0.375\n",
            "\n",
            "MLkNN com Grid Search:\n",
            "Melhores parâmetros = {'k': 9, 's': 1.0} \n",
            "Melhor score = 0.2\n",
            "Hamming Loss = 0.18382352941176472\n",
            "Acurácia = 0.16666666666666666\n",
            "Precisão = 0.5454545454545454\n",
            "Revocação = 0.375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxh5ECk1rN9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dab4e4b-5485-4740-fe16-e8319f39b959"
      },
      "source": [
        "# LP\n",
        "print(\"LP simples:\")\n",
        "LP_simples(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nLP com Grid Search:\")\n",
        "LP_Hamming_Loss_FoodTrucks , LP_Acuracia_FoodTrucks, LP_Precisão_FoodTrucks , LP_Revocação_FoodTrucks = LP_GS(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LP simples:\n",
            "Hamming Loss = 0.15849673202614378\n",
            "Acurácia = 0.28431372549019607\n",
            "Precisão = 0.6825396825396826\n",
            "Revocação = 0.35833333333333334\n",
            "\n",
            "LP com Grid Search:\n",
            "Melhores parâmetros = {'classifier': SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False), 'classifier__kernel': 'rbf'} \n",
            "Melhor score = 0.2918032786885246\n",
            "Hamming Loss = 0.16666666666666666\n",
            "Acurácia = 0.2647058823529412\n",
            "Precisão = 0.6764705882352942\n",
            "Revocação = 0.2875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG7kMOo8kdWL"
      },
      "source": [
        "labels = ['italian_food','brazilian_food','mexican_food','chinese_food','japanese_food','arabic_food','snacks','healthy_food','fitness_food','sweets_desserts','gourmet','street_food']\n",
        "\n",
        "y = df_food_test[labels]\n",
        "X = df_food_test.drop(labels,axis=1)\n",
        "\n",
        "#encoding de features categóricos\n",
        "X.replace({'dawn':1,'lunch':2,'afternoon':3,'happy_hour':4,'dinner':5,\n",
        "'F':1,'M':2,\n",
        "'divorced':1,'married':2,'single':3\n",
        "}, inplace=True)\n",
        "\n",
        "\n",
        "#Separação treino-teste\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42) \n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# o único feature que já não vem normalizado numa escala de 1 a 5 é a renda. Não sei se isso é um oversight ou intencional por parte do estudo mas eu decidi normalizar\n",
        "\n",
        "X_train = preprocessing.MinMaxScaler().fit_transform(X_train)\n",
        "X_test = preprocessing.MinMaxScaler().fit_transform(X_test)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00snS7Za2YCL"
      },
      "source": [
        "---\n",
        "## Birds\n",
        "\n",
        " Esse dataset (disponivel em http://mulan.sourceforge.net/datasets-mlc.html) contem dados captados no H. J. Andrews (HJA) Long-Term Experimental Research Forest, na cordilheira das cascatas . Nesse local, há anos, são captados audios para analises de diversos fatores, e com eles, é possivel detectar o tipo de clima, composição vegetal e muitas outras coisas.\n",
        "\n",
        "  Esse dataset em especifico foi feito para ser utilizado na nona competição MLSP anual e nele esta incluso chuva, vento e ate mesmo audios sem passaros. Ele possui 645 gravações de 10 segundos no formato WAV.\n",
        "\n",
        "  Há 19 especies de passáros presentes sendo elas:\n",
        "\n",
        " * Brown Creeper\n",
        " * Pacific Wren\n",
        " * Pacific-slope Flycatcher\n",
        " * Red-breasted Nuthatch\n",
        " * Dark-eyed Junco\n",
        " * Olive-sided Flycatcher\n",
        " * Hermit Thrush\n",
        " * Chestnut-backed Chickadee\n",
        " * Varied Thrush\n",
        " * Hermit Warbler\n",
        " * Swainson’s Thrush\n",
        " * Hammond’s Flycatcher\n",
        " * Western Tanager\n",
        " * Black-headed Grosbeak\n",
        " * Golden Crowned Kinglet\n",
        " * Warbling Vireo\n",
        " * MacGillivray’s Warbler\n",
        " * Stellar’s Jay\n",
        " * Common Nighthawk\n",
        "\n",
        "\n",
        "  Os audios foram marcados com a presença de um conjunto de passaros, detectados por uma equipe de especialistas após analisarem os audios e os espectrogramas.\n",
        "\n",
        "  O dataset está dividido em teste e treino, \n",
        "\n",
        "  Por fim, esse dataset foi utilizado na competição, e seus resultados foram exibidos na 2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING. Não tivemos acesso às soluções utilizadas na competição e nem utilizamos o relatorio que exibe as ideias utilizadas pelas vencedores, uma vez que esse trabalho tem a ver com a comparação dos metodos escolhidos por nós em 5 datasets diferentes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "znw47prTa8cx",
        "outputId": "6020557a-73d9-47ad-ba2f-37c43ce717d8"
      },
      "source": [
        "#Carregar o dataset de treino\n",
        "df_birds_train = pd.read_csv(\"https://raw.githubusercontent.com/LeoRyu18/AM2_T1/main/csv_result-birds-train.csv\")\n",
        "df_birds_train = df_birds_train.drop('id', axis = 1)\n",
        "df_birds_train.head()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audio-ssd1</th>\n",
              "      <th>audio-ssd2</th>\n",
              "      <th>audio-ssd3</th>\n",
              "      <th>audio-ssd4</th>\n",
              "      <th>audio-ssd5</th>\n",
              "      <th>audio-ssd6</th>\n",
              "      <th>audio-ssd7</th>\n",
              "      <th>audio-ssd8</th>\n",
              "      <th>audio-ssd9</th>\n",
              "      <th>audio-ssd10</th>\n",
              "      <th>audio-ssd11</th>\n",
              "      <th>audio-ssd12</th>\n",
              "      <th>audio-ssd13</th>\n",
              "      <th>audio-ssd14</th>\n",
              "      <th>audio-ssd15</th>\n",
              "      <th>audio-ssd16</th>\n",
              "      <th>audio-ssd17</th>\n",
              "      <th>audio-ssd18</th>\n",
              "      <th>audio-ssd19</th>\n",
              "      <th>audio-ssd20</th>\n",
              "      <th>audio-ssd21</th>\n",
              "      <th>audio-ssd22</th>\n",
              "      <th>audio-ssd25</th>\n",
              "      <th>audio-ssd26</th>\n",
              "      <th>audio-ssd27</th>\n",
              "      <th>audio-ssd28</th>\n",
              "      <th>audio-ssd29</th>\n",
              "      <th>audio-ssd30</th>\n",
              "      <th>audio-ssd31</th>\n",
              "      <th>audio-ssd32</th>\n",
              "      <th>audio-ssd33</th>\n",
              "      <th>audio-ssd34</th>\n",
              "      <th>audio-ssd35</th>\n",
              "      <th>audio-ssd36</th>\n",
              "      <th>audio-ssd37</th>\n",
              "      <th>audio-ssd38</th>\n",
              "      <th>audio-ssd39</th>\n",
              "      <th>audio-ssd40</th>\n",
              "      <th>audio-ssd41</th>\n",
              "      <th>audio-ssd42</th>\n",
              "      <th>...</th>\n",
              "      <th>cluster89</th>\n",
              "      <th>cluster90</th>\n",
              "      <th>cluster91</th>\n",
              "      <th>cluster92</th>\n",
              "      <th>cluster93</th>\n",
              "      <th>cluster94</th>\n",
              "      <th>cluster95</th>\n",
              "      <th>cluster96</th>\n",
              "      <th>cluster97</th>\n",
              "      <th>cluster98</th>\n",
              "      <th>cluster99</th>\n",
              "      <th>cluster100</th>\n",
              "      <th>segments</th>\n",
              "      <th>mean_rect_width</th>\n",
              "      <th>std_rect_width</th>\n",
              "      <th>mean_rect_height</th>\n",
              "      <th>std_rect_height</th>\n",
              "      <th>mean_rect_volume</th>\n",
              "      <th>std_rect_volume</th>\n",
              "      <th>hasSegments</th>\n",
              "      <th>location</th>\n",
              "      <th>Brown</th>\n",
              "      <th>Pacific</th>\n",
              "      <th>Pacific-slope</th>\n",
              "      <th>Red-breasted</th>\n",
              "      <th>Dark-eyed</th>\n",
              "      <th>Olive-sided</th>\n",
              "      <th>Hermit</th>\n",
              "      <th>Chestnut-backed</th>\n",
              "      <th>Varied</th>\n",
              "      <th>Hermit.1</th>\n",
              "      <th>Swainson</th>\n",
              "      <th>Hammond</th>\n",
              "      <th>Western</th>\n",
              "      <th>Black-headed</th>\n",
              "      <th>Golden</th>\n",
              "      <th>Warbling</th>\n",
              "      <th>MacGillivray</th>\n",
              "      <th>Stellar</th>\n",
              "      <th>Common</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.016521</td>\n",
              "      <td>0.039926</td>\n",
              "      <td>0.089632</td>\n",
              "      <td>0.134119</td>\n",
              "      <td>0.170470</td>\n",
              "      <td>0.176872</td>\n",
              "      <td>0.171546</td>\n",
              "      <td>0.182392</td>\n",
              "      <td>0.162482</td>\n",
              "      <td>0.159083</td>\n",
              "      <td>0.164531</td>\n",
              "      <td>0.163366</td>\n",
              "      <td>0.171633</td>\n",
              "      <td>0.219787</td>\n",
              "      <td>0.270805</td>\n",
              "      <td>0.339206</td>\n",
              "      <td>0.327098</td>\n",
              "      <td>0.264581</td>\n",
              "      <td>0.173363</td>\n",
              "      <td>0.131426</td>\n",
              "      <td>0.068158</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.001114</td>\n",
              "      <td>0.003419</td>\n",
              "      <td>0.004479</td>\n",
              "      <td>0.004231</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.003757</td>\n",
              "      <td>0.003314</td>\n",
              "      <td>0.003246</td>\n",
              "      <td>0.002308</td>\n",
              "      <td>0.002129</td>\n",
              "      <td>0.001777</td>\n",
              "      <td>0.002338</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.003741</td>\n",
              "      <td>0.005809</td>\n",
              "      <td>0.006852</td>\n",
              "      <td>0.004594</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13</td>\n",
              "      <td>16.384615</td>\n",
              "      <td>20.617394</td>\n",
              "      <td>46.769231</td>\n",
              "      <td>71.863118</td>\n",
              "      <td>788.923077</td>\n",
              "      <td>1761.802180</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.006600</td>\n",
              "      <td>0.035984</td>\n",
              "      <td>0.089956</td>\n",
              "      <td>0.123214</td>\n",
              "      <td>0.172273</td>\n",
              "      <td>0.177068</td>\n",
              "      <td>0.165507</td>\n",
              "      <td>0.179655</td>\n",
              "      <td>0.161744</td>\n",
              "      <td>0.163678</td>\n",
              "      <td>0.161606</td>\n",
              "      <td>0.159523</td>\n",
              "      <td>0.171042</td>\n",
              "      <td>0.217206</td>\n",
              "      <td>0.254929</td>\n",
              "      <td>0.307129</td>\n",
              "      <td>0.293592</td>\n",
              "      <td>0.242930</td>\n",
              "      <td>0.151817</td>\n",
              "      <td>0.105817</td>\n",
              "      <td>0.062566</td>\n",
              "      <td>0.001245</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.003037</td>\n",
              "      <td>0.004070</td>\n",
              "      <td>0.004311</td>\n",
              "      <td>0.004704</td>\n",
              "      <td>0.003967</td>\n",
              "      <td>0.003847</td>\n",
              "      <td>0.002940</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.001950</td>\n",
              "      <td>0.002063</td>\n",
              "      <td>0.002207</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>0.002521</td>\n",
              "      <td>0.001875</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.006894</td>\n",
              "      <td>0.017722</td>\n",
              "      <td>0.048062</td>\n",
              "      <td>0.065802</td>\n",
              "      <td>0.103443</td>\n",
              "      <td>0.091397</td>\n",
              "      <td>0.084931</td>\n",
              "      <td>0.088666</td>\n",
              "      <td>0.075676</td>\n",
              "      <td>0.074408</td>\n",
              "      <td>0.074683</td>\n",
              "      <td>0.083202</td>\n",
              "      <td>0.088820</td>\n",
              "      <td>0.125175</td>\n",
              "      <td>0.165580</td>\n",
              "      <td>0.212101</td>\n",
              "      <td>0.217109</td>\n",
              "      <td>0.153888</td>\n",
              "      <td>0.099709</td>\n",
              "      <td>0.074910</td>\n",
              "      <td>0.045928</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.001433</td>\n",
              "      <td>0.002134</td>\n",
              "      <td>0.002286</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001417</td>\n",
              "      <td>0.001420</td>\n",
              "      <td>0.001047</td>\n",
              "      <td>0.000855</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>0.001438</td>\n",
              "      <td>0.002761</td>\n",
              "      <td>0.005301</td>\n",
              "      <td>0.005453</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>2.828427</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>1.414214</td>\n",
              "      <td>674.000000</td>\n",
              "      <td>113.137085</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.031046</td>\n",
              "      <td>0.127675</td>\n",
              "      <td>0.221428</td>\n",
              "      <td>0.272707</td>\n",
              "      <td>0.358743</td>\n",
              "      <td>0.349389</td>\n",
              "      <td>0.316029</td>\n",
              "      <td>0.330656</td>\n",
              "      <td>0.310752</td>\n",
              "      <td>0.306288</td>\n",
              "      <td>0.300054</td>\n",
              "      <td>0.304569</td>\n",
              "      <td>0.295422</td>\n",
              "      <td>0.367728</td>\n",
              "      <td>0.398225</td>\n",
              "      <td>0.457381</td>\n",
              "      <td>0.429034</td>\n",
              "      <td>0.330248</td>\n",
              "      <td>0.213530</td>\n",
              "      <td>0.131256</td>\n",
              "      <td>0.075369</td>\n",
              "      <td>0.001508</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.005276</td>\n",
              "      <td>0.008511</td>\n",
              "      <td>0.010244</td>\n",
              "      <td>0.010371</td>\n",
              "      <td>0.009083</td>\n",
              "      <td>0.008407</td>\n",
              "      <td>0.006876</td>\n",
              "      <td>0.006512</td>\n",
              "      <td>0.005826</td>\n",
              "      <td>0.004873</td>\n",
              "      <td>0.004481</td>\n",
              "      <td>0.004114</td>\n",
              "      <td>0.004384</td>\n",
              "      <td>0.004423</td>\n",
              "      <td>0.004448</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>0.002198</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.064721</td>\n",
              "      <td>0.226644</td>\n",
              "      <td>0.304482</td>\n",
              "      <td>0.274662</td>\n",
              "      <td>0.346980</td>\n",
              "      <td>0.334063</td>\n",
              "      <td>0.307223</td>\n",
              "      <td>0.324666</td>\n",
              "      <td>0.297070</td>\n",
              "      <td>0.292258</td>\n",
              "      <td>0.287987</td>\n",
              "      <td>0.289535</td>\n",
              "      <td>0.286109</td>\n",
              "      <td>0.354479</td>\n",
              "      <td>0.389812</td>\n",
              "      <td>0.444778</td>\n",
              "      <td>0.407931</td>\n",
              "      <td>0.312861</td>\n",
              "      <td>0.197836</td>\n",
              "      <td>0.122053</td>\n",
              "      <td>0.069657</td>\n",
              "      <td>0.001287</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>0.015984</td>\n",
              "      <td>0.014081</td>\n",
              "      <td>0.008860</td>\n",
              "      <td>0.009464</td>\n",
              "      <td>0.008694</td>\n",
              "      <td>0.007648</td>\n",
              "      <td>0.007977</td>\n",
              "      <td>0.006727</td>\n",
              "      <td>0.005021</td>\n",
              "      <td>0.004376</td>\n",
              "      <td>0.004248</td>\n",
              "      <td>0.004151</td>\n",
              "      <td>0.004636</td>\n",
              "      <td>0.004249</td>\n",
              "      <td>0.004037</td>\n",
              "      <td>0.002778</td>\n",
              "      <td>0.001979</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 279 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   audio-ssd1  audio-ssd2  audio-ssd3  ...  MacGillivray  Stellar  Common\n",
              "0    0.016521    0.039926    0.089632  ...             0        0       0\n",
              "1    0.006600    0.035984    0.089956  ...             0        0       0\n",
              "2    0.006894    0.017722    0.048062  ...             0        0       0\n",
              "3    0.031046    0.127675    0.221428  ...             0        0       0\n",
              "4    0.064721    0.226644    0.304482  ...             0        0       0\n",
              "\n",
              "[5 rows x 279 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "aqmRUBXRfdx6",
        "outputId": "38bdcd4a-a66f-4b65-b237-2b2f2e901761"
      },
      "source": [
        "#Carregar o dataset de teste\n",
        "df_birds_test = pd.read_csv(\"https://raw.githubusercontent.com/LeoRyu18/AM2_T1/main/csv_result-birds-test.csv\")\n",
        "df_birds_test = df_birds_test.drop('id', axis = 1)\n",
        "df_birds_test.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audio-ssd1</th>\n",
              "      <th>audio-ssd2</th>\n",
              "      <th>audio-ssd3</th>\n",
              "      <th>audio-ssd4</th>\n",
              "      <th>audio-ssd5</th>\n",
              "      <th>audio-ssd6</th>\n",
              "      <th>audio-ssd7</th>\n",
              "      <th>audio-ssd8</th>\n",
              "      <th>audio-ssd9</th>\n",
              "      <th>audio-ssd10</th>\n",
              "      <th>audio-ssd11</th>\n",
              "      <th>audio-ssd12</th>\n",
              "      <th>audio-ssd13</th>\n",
              "      <th>audio-ssd14</th>\n",
              "      <th>audio-ssd15</th>\n",
              "      <th>audio-ssd16</th>\n",
              "      <th>audio-ssd17</th>\n",
              "      <th>audio-ssd18</th>\n",
              "      <th>audio-ssd19</th>\n",
              "      <th>audio-ssd20</th>\n",
              "      <th>audio-ssd21</th>\n",
              "      <th>audio-ssd22</th>\n",
              "      <th>audio-ssd25</th>\n",
              "      <th>audio-ssd26</th>\n",
              "      <th>audio-ssd27</th>\n",
              "      <th>audio-ssd28</th>\n",
              "      <th>audio-ssd29</th>\n",
              "      <th>audio-ssd30</th>\n",
              "      <th>audio-ssd31</th>\n",
              "      <th>audio-ssd32</th>\n",
              "      <th>audio-ssd33</th>\n",
              "      <th>audio-ssd34</th>\n",
              "      <th>audio-ssd35</th>\n",
              "      <th>audio-ssd36</th>\n",
              "      <th>audio-ssd37</th>\n",
              "      <th>audio-ssd38</th>\n",
              "      <th>audio-ssd39</th>\n",
              "      <th>audio-ssd40</th>\n",
              "      <th>audio-ssd41</th>\n",
              "      <th>audio-ssd42</th>\n",
              "      <th>...</th>\n",
              "      <th>cluster89</th>\n",
              "      <th>cluster90</th>\n",
              "      <th>cluster91</th>\n",
              "      <th>cluster92</th>\n",
              "      <th>cluster93</th>\n",
              "      <th>cluster94</th>\n",
              "      <th>cluster95</th>\n",
              "      <th>cluster96</th>\n",
              "      <th>cluster97</th>\n",
              "      <th>cluster98</th>\n",
              "      <th>cluster99</th>\n",
              "      <th>cluster100</th>\n",
              "      <th>segments</th>\n",
              "      <th>mean_rect_width</th>\n",
              "      <th>std_rect_width</th>\n",
              "      <th>mean_rect_height</th>\n",
              "      <th>std_rect_height</th>\n",
              "      <th>mean_rect_volume</th>\n",
              "      <th>std_rect_volume</th>\n",
              "      <th>hasSegments</th>\n",
              "      <th>location</th>\n",
              "      <th>Brown</th>\n",
              "      <th>Pacific</th>\n",
              "      <th>Pacific-slope</th>\n",
              "      <th>Red-breasted</th>\n",
              "      <th>Dark-eyed</th>\n",
              "      <th>Olive-sided</th>\n",
              "      <th>Hermit</th>\n",
              "      <th>Chestnut-backed</th>\n",
              "      <th>Varied</th>\n",
              "      <th>Hermit.1</th>\n",
              "      <th>Swainson</th>\n",
              "      <th>Hammond</th>\n",
              "      <th>Western</th>\n",
              "      <th>Black-headed</th>\n",
              "      <th>Golden</th>\n",
              "      <th>Warbling</th>\n",
              "      <th>MacGillivray</th>\n",
              "      <th>Stellar</th>\n",
              "      <th>Common</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.132445</td>\n",
              "      <td>0.143931</td>\n",
              "      <td>0.227729</td>\n",
              "      <td>0.298556</td>\n",
              "      <td>0.385907</td>\n",
              "      <td>0.378363</td>\n",
              "      <td>0.354708</td>\n",
              "      <td>0.384165</td>\n",
              "      <td>0.360092</td>\n",
              "      <td>0.347465</td>\n",
              "      <td>0.341827</td>\n",
              "      <td>0.349941</td>\n",
              "      <td>0.349431</td>\n",
              "      <td>0.425508</td>\n",
              "      <td>0.470748</td>\n",
              "      <td>0.545955</td>\n",
              "      <td>0.515480</td>\n",
              "      <td>0.439492</td>\n",
              "      <td>0.287173</td>\n",
              "      <td>0.134652</td>\n",
              "      <td>0.082536</td>\n",
              "      <td>0.001778</td>\n",
              "      <td>0.009794</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.008890</td>\n",
              "      <td>0.011114</td>\n",
              "      <td>0.010665</td>\n",
              "      <td>0.010292</td>\n",
              "      <td>0.008779</td>\n",
              "      <td>0.008474</td>\n",
              "      <td>0.008938</td>\n",
              "      <td>0.006323</td>\n",
              "      <td>0.006312</td>\n",
              "      <td>0.005450</td>\n",
              "      <td>0.005856</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.005768</td>\n",
              "      <td>0.005577</td>\n",
              "      <td>0.004093</td>\n",
              "      <td>0.002956</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.101617</td>\n",
              "      <td>0.130342</td>\n",
              "      <td>0.228117</td>\n",
              "      <td>0.281017</td>\n",
              "      <td>0.365804</td>\n",
              "      <td>0.370122</td>\n",
              "      <td>0.359235</td>\n",
              "      <td>0.388608</td>\n",
              "      <td>0.362013</td>\n",
              "      <td>0.348229</td>\n",
              "      <td>0.342542</td>\n",
              "      <td>0.345851</td>\n",
              "      <td>0.338571</td>\n",
              "      <td>0.424733</td>\n",
              "      <td>0.470891</td>\n",
              "      <td>0.547948</td>\n",
              "      <td>0.516554</td>\n",
              "      <td>0.445265</td>\n",
              "      <td>0.293809</td>\n",
              "      <td>0.140007</td>\n",
              "      <td>0.088689</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.005568</td>\n",
              "      <td>0.006061</td>\n",
              "      <td>0.008843</td>\n",
              "      <td>0.009781</td>\n",
              "      <td>0.009482</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.011016</td>\n",
              "      <td>0.009763</td>\n",
              "      <td>0.007648</td>\n",
              "      <td>0.006345</td>\n",
              "      <td>0.005849</td>\n",
              "      <td>0.005672</td>\n",
              "      <td>0.005793</td>\n",
              "      <td>0.005648</td>\n",
              "      <td>0.005650</td>\n",
              "      <td>0.005195</td>\n",
              "      <td>0.003621</td>\n",
              "      <td>0.003111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.005148</td>\n",
              "      <td>0.017877</td>\n",
              "      <td>0.042137</td>\n",
              "      <td>0.062124</td>\n",
              "      <td>0.097340</td>\n",
              "      <td>0.088305</td>\n",
              "      <td>0.084337</td>\n",
              "      <td>0.083204</td>\n",
              "      <td>0.074532</td>\n",
              "      <td>0.071497</td>\n",
              "      <td>0.074953</td>\n",
              "      <td>0.077544</td>\n",
              "      <td>0.086848</td>\n",
              "      <td>0.126177</td>\n",
              "      <td>0.209470</td>\n",
              "      <td>0.275132</td>\n",
              "      <td>0.270673</td>\n",
              "      <td>0.238987</td>\n",
              "      <td>0.229216</td>\n",
              "      <td>0.194186</td>\n",
              "      <td>0.094694</td>\n",
              "      <td>0.001207</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>0.001294</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.002196</td>\n",
              "      <td>0.001996</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>0.001442</td>\n",
              "      <td>0.001275</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.000609</td>\n",
              "      <td>0.000805</td>\n",
              "      <td>0.001435</td>\n",
              "      <td>0.017121</td>\n",
              "      <td>0.022838</td>\n",
              "      <td>0.013595</td>\n",
              "      <td>0.026657</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17</td>\n",
              "      <td>27.235294</td>\n",
              "      <td>44.822526</td>\n",
              "      <td>91.647059</td>\n",
              "      <td>199.564231</td>\n",
              "      <td>2939.823529</td>\n",
              "      <td>9015.550592</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.018792</td>\n",
              "      <td>0.012898</td>\n",
              "      <td>0.027330</td>\n",
              "      <td>0.039521</td>\n",
              "      <td>0.064671</td>\n",
              "      <td>0.068329</td>\n",
              "      <td>0.065799</td>\n",
              "      <td>0.059891</td>\n",
              "      <td>0.048287</td>\n",
              "      <td>0.047820</td>\n",
              "      <td>0.049324</td>\n",
              "      <td>0.055350</td>\n",
              "      <td>0.060587</td>\n",
              "      <td>0.086072</td>\n",
              "      <td>0.111716</td>\n",
              "      <td>0.150919</td>\n",
              "      <td>0.170447</td>\n",
              "      <td>0.133900</td>\n",
              "      <td>0.089051</td>\n",
              "      <td>0.068235</td>\n",
              "      <td>0.043178</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.001437</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.000785</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.000987</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.007008</td>\n",
              "      <td>0.014610</td>\n",
              "      <td>0.033637</td>\n",
              "      <td>0.042604</td>\n",
              "      <td>0.065649</td>\n",
              "      <td>0.065047</td>\n",
              "      <td>0.064553</td>\n",
              "      <td>0.058155</td>\n",
              "      <td>0.048516</td>\n",
              "      <td>0.047021</td>\n",
              "      <td>0.050417</td>\n",
              "      <td>0.054324</td>\n",
              "      <td>0.059212</td>\n",
              "      <td>0.086488</td>\n",
              "      <td>0.110848</td>\n",
              "      <td>0.147765</td>\n",
              "      <td>0.163671</td>\n",
              "      <td>0.130540</td>\n",
              "      <td>0.088572</td>\n",
              "      <td>0.068011</td>\n",
              "      <td>0.043039</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000833</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.001409</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.000874</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.000554</td>\n",
              "      <td>0.000733</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.000948</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 279 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   audio-ssd1  audio-ssd2  audio-ssd3  ...  MacGillivray  Stellar  Common\n",
              "0    0.132445    0.143931    0.227729  ...             0        0       0\n",
              "1    0.101617    0.130342    0.228117  ...             0        0       0\n",
              "2    0.005148    0.017877    0.042137  ...             0        0       0\n",
              "3    0.018792    0.012898    0.027330  ...             0        0       0\n",
              "4    0.007008    0.014610    0.033637  ...             0        0       0\n",
              "\n",
              "[5 rows x 279 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYTOdcXybucJ"
      },
      "source": [
        "#dividir em x e y\n",
        "labels =  ['Brown', 'Pacific', 'Pacific-slope', 'Red-breasted', ' Dark-eyed', 'Olive-sided', 'Hermit', 'Chestnut-backed', 'Varied', 'Hermit', 'Swainson', 'Hammond', 'Western', 'Black-headed', 'Golden', 'Warbling', 'MacGillivray', 'Stellar', 'Common']\n",
        "\n",
        "y_train_bird = np.array(df_birds_train[labels])\n",
        "y_test_bird = np.array(df_birds_test[labels])\n",
        "\n",
        "X_train = df_birds_train.drop(labels,axis=1)\n",
        "X_test = df_birds_test.drop(labels,axis=1)\n",
        "\n",
        "#normalizar\n",
        "X_train = preprocessing.MinMaxScaler().fit_transform(X_train)\n",
        "X_test = preprocessing.MinMaxScaler().fit_transform(X_test)\n",
        "\n",
        "X_train_bird = np.array(X_train)\n",
        "X_test_bird = np.array(X_test)\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koZjshVCcqM5",
        "outputId": "574e2ac8-d921-4bbd-af53-7c4a07c08e06"
      },
      "source": [
        "print(\"BR simples:\")\n",
        "BR_simples(X_train_bird, y_train_bird, X_test_bird, y_test_bird)\n",
        "\n",
        "print(\"\\nBR com Grid Search:\")\n",
        "BR_Hamming_Loss_birds , BR_Acuracia_birds, BR_Precisão_birds , BR_Revocação_birds = BR_GS(X_train_bird, y_train_bird, X_test_bird, y_test_bird)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BR simples:\n",
            "Hamming Loss = 0.04497311389929933\n",
            "Acurácia = 0.5201238390092879\n",
            "Precisão = 0.9428571428571428\n",
            "Revocação = 0.10749185667752444\n",
            "\n",
            "BR com Grid Search:\n",
            "Melhores parâmetros = {'classifier': SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False), 'classifier__kernel': 'linear'} \n",
            "Melhor score = 0.5279807692307692\n",
            "Hamming Loss = 0.048069089131497475\n",
            "Acurácia = 0.5139318885448917\n",
            "Precisão = 0.5267857142857143\n",
            "Revocação = 0.38436482084690554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDBZKKQIcwow",
        "outputId": "c4242755-3976-4c26-be8f-8a9ef496a79f"
      },
      "source": [
        "# MLkNN\n",
        "print(\"MLkNN simples:\")\n",
        "MLkNN_simples(X_train_bird, y_train_bird, X_test_bird, y_test_bird)\n",
        "\n",
        "print(\"\\nMLkNN com Grid Search:\")\n",
        "MLkNN_Hamming_Loss_birds , MLkNN_Acuracia_birds, MLkNN , MLkNN_Revocação_birds = MLkNN_GS(X_train_bird, y_train_bird, X_test_bird, y_test_bird)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLkNN simples:\n",
            "Hamming Loss = 0.0472543588072348\n",
            "Acurácia = 0.5263157894736842\n",
            "Precisão = 0.5459459459459459\n",
            "Revocação = 0.3289902280130293\n",
            "\n",
            "MLkNN com Grid Search:\n",
            "Melhores parâmetros = {'k': 7, 's': 1.0} \n",
            "Melhor score = 0.5031730769230769\n",
            "Hamming Loss = 0.046928466677529736\n",
            "Acurácia = 0.5108359133126935\n",
            "Precisão = 0.5871559633027523\n",
            "Revocação = 0.20846905537459284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsbEvBjXc1dZ",
        "outputId": "38419492-94df-473a-99f1-e0ac4c870d0a"
      },
      "source": [
        "# LP\n",
        "print(\"LP simples:\")\n",
        "LP_simples(X_train_bird, y_train_bird, X_test_bird, y_test_bird)\n",
        "\n",
        "print(\"\\nLP com Grid Search:\")\n",
        "LP_Hamming_Loss_birds , LP_Acuracia_birds, LP_Precisão_birds , LP_Revocação_birds = LP_GS(X_train_bird, y_train_bird, X_test_bird, y_test_bird)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LP simples:\n",
            "Hamming Loss = 0.03943294769431318\n",
            "Acurácia = 0.5386996904024768\n",
            "Precisão = 0.8217821782178217\n",
            "Revocação = 0.2703583061889251\n",
            "\n",
            "LP com Grid Search:\n",
            "Melhores parâmetros = {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False), 'classifier__criterion': 'gini', 'classifier__n_estimators': 50} \n",
            "Melhor score = 0.5404326923076923\n",
            "Hamming Loss = 0.042203030796806255\n",
            "Acurácia = 0.5541795665634675\n",
            "Precisão = 0.6363636363636364\n",
            "Revocação = 0.36482084690553745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyWZaeClJ1G7"
      },
      "source": [
        "# Scene\n",
        "Dataset obtido em https://sites.google.com/site/hrsvmproject/datasets-multi\n",
        "\n",
        "O dataset Scene é composto por 2407  imagens que podem possuir 6 classes diferentes, essas classes são: Beach, Sunset, FallFoliage, Field, Mountain e Urban. As imagens foram convertidas para o espaço de cores LUV, e após isso elas são divididas em 49 blocos usando uma grade 7x7. Primeiramente é calculado a média dos 49 blocos,  e após isso é calculado a média para cada bloco, e após isso é calculado a variância para cada orientação. Dessa forma temos 294 features para cada imagem do dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzYXOi8YJ85f"
      },
      "source": [
        "df_scene_test =  pd.read_csv(\"https://raw.githubusercontent.com/LeoRyu18/AM2_T1/main/Scene/csv_result-scene-test.csv\")\n",
        "df_scene_test = df_scene_test.drop('id', axis = 1)\n",
        "\n",
        "df_scene_train = pd.read_csv(\"https://raw.githubusercontent.com/LeoRyu18/AM2_T1/main/Scene/csv_result-scene-train.csv\")\n",
        "df_scene_train = df_scene_train.drop('id', axis = 1)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "6fZe8is0Kkiy",
        "outputId": "200f37b3-2507-49f3-c89d-0264b4e71b9c"
      },
      "source": [
        "df_scene_train.head()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Att1</th>\n",
              "      <th>Att2</th>\n",
              "      <th>Att3</th>\n",
              "      <th>Att4</th>\n",
              "      <th>Att5</th>\n",
              "      <th>Att6</th>\n",
              "      <th>Att7</th>\n",
              "      <th>Att8</th>\n",
              "      <th>Att9</th>\n",
              "      <th>Att10</th>\n",
              "      <th>Att11</th>\n",
              "      <th>Att12</th>\n",
              "      <th>Att13</th>\n",
              "      <th>Att14</th>\n",
              "      <th>Att15</th>\n",
              "      <th>Att16</th>\n",
              "      <th>Att17</th>\n",
              "      <th>Att18</th>\n",
              "      <th>Att19</th>\n",
              "      <th>Att20</th>\n",
              "      <th>Att21</th>\n",
              "      <th>Att22</th>\n",
              "      <th>Att23</th>\n",
              "      <th>Att24</th>\n",
              "      <th>Att25</th>\n",
              "      <th>Att26</th>\n",
              "      <th>Att27</th>\n",
              "      <th>Att28</th>\n",
              "      <th>Att29</th>\n",
              "      <th>Att30</th>\n",
              "      <th>Att31</th>\n",
              "      <th>Att32</th>\n",
              "      <th>Att33</th>\n",
              "      <th>Att34</th>\n",
              "      <th>Att35</th>\n",
              "      <th>Att36</th>\n",
              "      <th>Att37</th>\n",
              "      <th>Att38</th>\n",
              "      <th>Att39</th>\n",
              "      <th>Att40</th>\n",
              "      <th>...</th>\n",
              "      <th>Att261</th>\n",
              "      <th>Att262</th>\n",
              "      <th>Att263</th>\n",
              "      <th>Att264</th>\n",
              "      <th>Att265</th>\n",
              "      <th>Att266</th>\n",
              "      <th>Att267</th>\n",
              "      <th>Att268</th>\n",
              "      <th>Att269</th>\n",
              "      <th>Att270</th>\n",
              "      <th>Att271</th>\n",
              "      <th>Att272</th>\n",
              "      <th>Att273</th>\n",
              "      <th>Att274</th>\n",
              "      <th>Att275</th>\n",
              "      <th>Att276</th>\n",
              "      <th>Att277</th>\n",
              "      <th>Att278</th>\n",
              "      <th>Att279</th>\n",
              "      <th>Att280</th>\n",
              "      <th>Att281</th>\n",
              "      <th>Att282</th>\n",
              "      <th>Att283</th>\n",
              "      <th>Att284</th>\n",
              "      <th>Att285</th>\n",
              "      <th>Att286</th>\n",
              "      <th>Att287</th>\n",
              "      <th>Att288</th>\n",
              "      <th>Att289</th>\n",
              "      <th>Att290</th>\n",
              "      <th>Att291</th>\n",
              "      <th>Att292</th>\n",
              "      <th>Att293</th>\n",
              "      <th>Att294</th>\n",
              "      <th>Beach</th>\n",
              "      <th>Sunset</th>\n",
              "      <th>FallFoliage</th>\n",
              "      <th>Field</th>\n",
              "      <th>Mountain</th>\n",
              "      <th>Urban</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.646467</td>\n",
              "      <td>0.666435</td>\n",
              "      <td>0.685047</td>\n",
              "      <td>0.699053</td>\n",
              "      <td>0.652746</td>\n",
              "      <td>0.407864</td>\n",
              "      <td>0.150309</td>\n",
              "      <td>0.535193</td>\n",
              "      <td>0.555689</td>\n",
              "      <td>0.580782</td>\n",
              "      <td>0.577094</td>\n",
              "      <td>0.390455</td>\n",
              "      <td>0.242458</td>\n",
              "      <td>0.170217</td>\n",
              "      <td>0.421797</td>\n",
              "      <td>0.428206</td>\n",
              "      <td>0.428277</td>\n",
              "      <td>0.490017</td>\n",
              "      <td>0.459252</td>\n",
              "      <td>0.350897</td>\n",
              "      <td>0.255987</td>\n",
              "      <td>0.310109</td>\n",
              "      <td>0.375018</td>\n",
              "      <td>0.437369</td>\n",
              "      <td>0.451752</td>\n",
              "      <td>0.508325</td>\n",
              "      <td>0.467347</td>\n",
              "      <td>0.567068</td>\n",
              "      <td>0.546262</td>\n",
              "      <td>0.566969</td>\n",
              "      <td>0.612951</td>\n",
              "      <td>0.621101</td>\n",
              "      <td>0.653561</td>\n",
              "      <td>0.694546</td>\n",
              "      <td>0.574777</td>\n",
              "      <td>0.710196</td>\n",
              "      <td>0.614510</td>\n",
              "      <td>0.590450</td>\n",
              "      <td>0.508313</td>\n",
              "      <td>0.645884</td>\n",
              "      <td>...</td>\n",
              "      <td>0.069331</td>\n",
              "      <td>0.136652</td>\n",
              "      <td>0.136285</td>\n",
              "      <td>0.127585</td>\n",
              "      <td>0.249868</td>\n",
              "      <td>0.545665</td>\n",
              "      <td>0.252143</td>\n",
              "      <td>0.261571</td>\n",
              "      <td>0.203095</td>\n",
              "      <td>0.172747</td>\n",
              "      <td>0.239030</td>\n",
              "      <td>0.309251</td>\n",
              "      <td>0.090241</td>\n",
              "      <td>0.048767</td>\n",
              "      <td>0.085062</td>\n",
              "      <td>0.072274</td>\n",
              "      <td>0.167601</td>\n",
              "      <td>0.094636</td>\n",
              "      <td>0.258751</td>\n",
              "      <td>0.092845</td>\n",
              "      <td>0.477150</td>\n",
              "      <td>0.224848</td>\n",
              "      <td>0.102568</td>\n",
              "      <td>0.329816</td>\n",
              "      <td>0.061538</td>\n",
              "      <td>0.049615</td>\n",
              "      <td>0.068962</td>\n",
              "      <td>0.653879</td>\n",
              "      <td>0.354982</td>\n",
              "      <td>0.124074</td>\n",
              "      <td>0.157332</td>\n",
              "      <td>0.247298</td>\n",
              "      <td>0.014025</td>\n",
              "      <td>0.029709</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.770156</td>\n",
              "      <td>0.767255</td>\n",
              "      <td>0.761053</td>\n",
              "      <td>0.745630</td>\n",
              "      <td>0.742231</td>\n",
              "      <td>0.688086</td>\n",
              "      <td>0.708416</td>\n",
              "      <td>0.757351</td>\n",
              "      <td>0.760633</td>\n",
              "      <td>0.740314</td>\n",
              "      <td>0.513377</td>\n",
              "      <td>0.600421</td>\n",
              "      <td>0.542340</td>\n",
              "      <td>0.439594</td>\n",
              "      <td>0.604272</td>\n",
              "      <td>0.624697</td>\n",
              "      <td>0.642823</td>\n",
              "      <td>0.424883</td>\n",
              "      <td>0.448578</td>\n",
              "      <td>0.318076</td>\n",
              "      <td>0.209851</td>\n",
              "      <td>0.570696</td>\n",
              "      <td>0.599071</td>\n",
              "      <td>0.556610</td>\n",
              "      <td>0.556215</td>\n",
              "      <td>0.653352</td>\n",
              "      <td>0.559962</td>\n",
              "      <td>0.473784</td>\n",
              "      <td>0.636677</td>\n",
              "      <td>0.653249</td>\n",
              "      <td>0.621813</td>\n",
              "      <td>0.613890</td>\n",
              "      <td>0.596795</td>\n",
              "      <td>0.596297</td>\n",
              "      <td>0.692224</td>\n",
              "      <td>0.634007</td>\n",
              "      <td>0.605896</td>\n",
              "      <td>0.594992</td>\n",
              "      <td>0.650470</td>\n",
              "      <td>0.582844</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088278</td>\n",
              "      <td>0.097577</td>\n",
              "      <td>0.167246</td>\n",
              "      <td>0.193839</td>\n",
              "      <td>0.283507</td>\n",
              "      <td>0.190554</td>\n",
              "      <td>0.072342</td>\n",
              "      <td>0.111906</td>\n",
              "      <td>0.175488</td>\n",
              "      <td>0.178064</td>\n",
              "      <td>0.249890</td>\n",
              "      <td>0.085085</td>\n",
              "      <td>0.073259</td>\n",
              "      <td>0.133331</td>\n",
              "      <td>0.090761</td>\n",
              "      <td>0.138334</td>\n",
              "      <td>0.102932</td>\n",
              "      <td>0.406639</td>\n",
              "      <td>0.126982</td>\n",
              "      <td>0.046562</td>\n",
              "      <td>0.354085</td>\n",
              "      <td>0.199359</td>\n",
              "      <td>0.157326</td>\n",
              "      <td>0.051859</td>\n",
              "      <td>0.114123</td>\n",
              "      <td>0.160008</td>\n",
              "      <td>0.414088</td>\n",
              "      <td>0.361843</td>\n",
              "      <td>0.303399</td>\n",
              "      <td>0.176387</td>\n",
              "      <td>0.251454</td>\n",
              "      <td>0.137833</td>\n",
              "      <td>0.082672</td>\n",
              "      <td>0.036320</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.793984</td>\n",
              "      <td>0.772096</td>\n",
              "      <td>0.761820</td>\n",
              "      <td>0.762213</td>\n",
              "      <td>0.740569</td>\n",
              "      <td>0.734361</td>\n",
              "      <td>0.722677</td>\n",
              "      <td>0.849128</td>\n",
              "      <td>0.839607</td>\n",
              "      <td>0.812746</td>\n",
              "      <td>0.785767</td>\n",
              "      <td>0.760288</td>\n",
              "      <td>0.751835</td>\n",
              "      <td>0.754508</td>\n",
              "      <td>0.853808</td>\n",
              "      <td>0.857499</td>\n",
              "      <td>0.858505</td>\n",
              "      <td>0.864827</td>\n",
              "      <td>0.865957</td>\n",
              "      <td>0.867185</td>\n",
              "      <td>0.872483</td>\n",
              "      <td>0.955915</td>\n",
              "      <td>0.966291</td>\n",
              "      <td>0.968941</td>\n",
              "      <td>0.879657</td>\n",
              "      <td>0.716114</td>\n",
              "      <td>0.479571</td>\n",
              "      <td>0.402155</td>\n",
              "      <td>0.754620</td>\n",
              "      <td>0.775176</td>\n",
              "      <td>0.723823</td>\n",
              "      <td>0.676656</td>\n",
              "      <td>0.633313</td>\n",
              "      <td>0.552341</td>\n",
              "      <td>0.417900</td>\n",
              "      <td>0.622198</td>\n",
              "      <td>0.652387</td>\n",
              "      <td>0.648123</td>\n",
              "      <td>0.680452</td>\n",
              "      <td>0.662322</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082237</td>\n",
              "      <td>0.060296</td>\n",
              "      <td>0.058945</td>\n",
              "      <td>0.052964</td>\n",
              "      <td>0.062245</td>\n",
              "      <td>0.075563</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.006033</td>\n",
              "      <td>0.181837</td>\n",
              "      <td>0.213608</td>\n",
              "      <td>0.122532</td>\n",
              "      <td>0.035184</td>\n",
              "      <td>0.025505</td>\n",
              "      <td>0.027821</td>\n",
              "      <td>0.353377</td>\n",
              "      <td>0.073733</td>\n",
              "      <td>0.048943</td>\n",
              "      <td>0.080248</td>\n",
              "      <td>0.074113</td>\n",
              "      <td>0.051372</td>\n",
              "      <td>0.024035</td>\n",
              "      <td>0.015971</td>\n",
              "      <td>0.028559</td>\n",
              "      <td>0.047596</td>\n",
              "      <td>0.038082</td>\n",
              "      <td>0.079977</td>\n",
              "      <td>0.004901</td>\n",
              "      <td>0.003460</td>\n",
              "      <td>0.006049</td>\n",
              "      <td>0.017166</td>\n",
              "      <td>0.051125</td>\n",
              "      <td>0.112506</td>\n",
              "      <td>0.083924</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.938563</td>\n",
              "      <td>0.949260</td>\n",
              "      <td>0.955621</td>\n",
              "      <td>0.966743</td>\n",
              "      <td>0.968649</td>\n",
              "      <td>0.869619</td>\n",
              "      <td>0.696925</td>\n",
              "      <td>0.953460</td>\n",
              "      <td>0.959631</td>\n",
              "      <td>0.966320</td>\n",
              "      <td>0.972766</td>\n",
              "      <td>0.916497</td>\n",
              "      <td>0.622508</td>\n",
              "      <td>0.530428</td>\n",
              "      <td>0.963539</td>\n",
              "      <td>0.972303</td>\n",
              "      <td>0.972980</td>\n",
              "      <td>0.945388</td>\n",
              "      <td>0.609497</td>\n",
              "      <td>0.514073</td>\n",
              "      <td>0.360757</td>\n",
              "      <td>0.804240</td>\n",
              "      <td>0.827367</td>\n",
              "      <td>0.813407</td>\n",
              "      <td>0.796413</td>\n",
              "      <td>0.753638</td>\n",
              "      <td>0.696435</td>\n",
              "      <td>0.520342</td>\n",
              "      <td>0.782931</td>\n",
              "      <td>0.774347</td>\n",
              "      <td>0.750613</td>\n",
              "      <td>0.706845</td>\n",
              "      <td>0.612971</td>\n",
              "      <td>0.647101</td>\n",
              "      <td>0.645833</td>\n",
              "      <td>0.736683</td>\n",
              "      <td>0.719352</td>\n",
              "      <td>0.643989</td>\n",
              "      <td>0.705878</td>\n",
              "      <td>0.773725</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000563</td>\n",
              "      <td>0.008393</td>\n",
              "      <td>0.093743</td>\n",
              "      <td>0.105665</td>\n",
              "      <td>0.060825</td>\n",
              "      <td>0.025972</td>\n",
              "      <td>0.045153</td>\n",
              "      <td>0.039900</td>\n",
              "      <td>0.030980</td>\n",
              "      <td>0.448542</td>\n",
              "      <td>0.024508</td>\n",
              "      <td>0.024751</td>\n",
              "      <td>0.045848</td>\n",
              "      <td>0.020989</td>\n",
              "      <td>0.015197</td>\n",
              "      <td>0.209978</td>\n",
              "      <td>0.138788</td>\n",
              "      <td>0.031173</td>\n",
              "      <td>0.032565</td>\n",
              "      <td>0.034237</td>\n",
              "      <td>0.018757</td>\n",
              "      <td>0.082271</td>\n",
              "      <td>0.201563</td>\n",
              "      <td>0.043669</td>\n",
              "      <td>0.027527</td>\n",
              "      <td>0.016922</td>\n",
              "      <td>0.024174</td>\n",
              "      <td>0.036799</td>\n",
              "      <td>0.007694</td>\n",
              "      <td>0.009735</td>\n",
              "      <td>0.019267</td>\n",
              "      <td>0.031290</td>\n",
              "      <td>0.049780</td>\n",
              "      <td>0.090959</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.512130</td>\n",
              "      <td>0.524684</td>\n",
              "      <td>0.520020</td>\n",
              "      <td>0.504467</td>\n",
              "      <td>0.471209</td>\n",
              "      <td>0.417654</td>\n",
              "      <td>0.364292</td>\n",
              "      <td>0.562266</td>\n",
              "      <td>0.588592</td>\n",
              "      <td>0.584449</td>\n",
              "      <td>0.570074</td>\n",
              "      <td>0.551043</td>\n",
              "      <td>0.503925</td>\n",
              "      <td>0.447526</td>\n",
              "      <td>0.500117</td>\n",
              "      <td>0.539517</td>\n",
              "      <td>0.588721</td>\n",
              "      <td>0.600226</td>\n",
              "      <td>0.588937</td>\n",
              "      <td>0.562027</td>\n",
              "      <td>0.510786</td>\n",
              "      <td>0.465298</td>\n",
              "      <td>0.626580</td>\n",
              "      <td>0.649661</td>\n",
              "      <td>0.629969</td>\n",
              "      <td>0.574756</td>\n",
              "      <td>0.519651</td>\n",
              "      <td>0.445292</td>\n",
              "      <td>0.450048</td>\n",
              "      <td>0.742275</td>\n",
              "      <td>0.784539</td>\n",
              "      <td>0.903786</td>\n",
              "      <td>0.834243</td>\n",
              "      <td>0.766266</td>\n",
              "      <td>0.657113</td>\n",
              "      <td>0.276264</td>\n",
              "      <td>0.394086</td>\n",
              "      <td>0.610411</td>\n",
              "      <td>0.698119</td>\n",
              "      <td>0.743710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.439771</td>\n",
              "      <td>0.595821</td>\n",
              "      <td>0.207690</td>\n",
              "      <td>0.028206</td>\n",
              "      <td>0.010644</td>\n",
              "      <td>0.010589</td>\n",
              "      <td>0.138157</td>\n",
              "      <td>0.094097</td>\n",
              "      <td>0.044848</td>\n",
              "      <td>0.036629</td>\n",
              "      <td>0.046537</td>\n",
              "      <td>0.090652</td>\n",
              "      <td>0.086531</td>\n",
              "      <td>0.293732</td>\n",
              "      <td>0.221770</td>\n",
              "      <td>0.094467</td>\n",
              "      <td>0.143500</td>\n",
              "      <td>0.186763</td>\n",
              "      <td>0.074600</td>\n",
              "      <td>0.043375</td>\n",
              "      <td>0.208570</td>\n",
              "      <td>0.188324</td>\n",
              "      <td>0.413413</td>\n",
              "      <td>0.387559</td>\n",
              "      <td>0.158730</td>\n",
              "      <td>0.023177</td>\n",
              "      <td>0.129994</td>\n",
              "      <td>0.167709</td>\n",
              "      <td>0.226580</td>\n",
              "      <td>0.218534</td>\n",
              "      <td>0.198151</td>\n",
              "      <td>0.238796</td>\n",
              "      <td>0.164270</td>\n",
              "      <td>0.184290</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Att1      Att2      Att3      Att4  ...  FallFoliage  Field  Mountain  Urban\n",
              "0  0.646467  0.666435  0.685047  0.699053  ...            0      0         1      0\n",
              "1  0.770156  0.767255  0.761053  0.745630  ...            0      0         0      1\n",
              "2  0.793984  0.772096  0.761820  0.762213  ...            0      0         0      0\n",
              "3  0.938563  0.949260  0.955621  0.966743  ...            0      0         0      0\n",
              "4  0.512130  0.524684  0.520020  0.504467  ...            0      0         0      0\n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piK2Mm0yNMWF"
      },
      "source": [
        "labels = [\"Beach\",\"Sunset\",\"FallFoliage\",\"Field\",\"Mountain\",\"Urban\"]\n",
        "\n",
        "y_train_scene = np.asarray(df_scene_train[labels])\n",
        "y_test_scene = np.asarray(df_scene_test[labels])\n",
        "\n",
        "x_train_scene = df_scene_train.drop(labels=labels,axis=1)\n",
        "x_test_scene = df_scene_test.drop(labels=labels,axis=1)\n",
        "\n",
        "x_train_scene = np.array(x_train_scene)\n",
        "x_test_scene = np.array(x_test_scene)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udRUgfvzPH-Q",
        "outputId": "ed555cd5-722e-43c0-e32a-3e3e0ee7b690"
      },
      "source": [
        "# BR\n",
        "print(\"BR simples:\")\n",
        "BR_simples(x_train_scene, y_train_scene, x_test_scene, y_test_scene)\n",
        "\n",
        "print(\"\\nBR com Grid Search:\")\n",
        "BR_Hamming_Loss_scene , BR_Acuracia_scene, BR_Precisão_scene , BR_Revocação_scene = BR_GS(x_train_scene, y_train_scene, x_test_scene, y_test_scene)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BR simples:\n",
            "Hamming Loss = 0.08416945373467112\n",
            "Acurácia = 0.5869565217391305\n",
            "Precisão = 0.8856825749167592\n",
            "Revocação = 0.6143187066974596\n",
            "\n",
            "BR com Grid Search:\n",
            "Melhores parâmetros = {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='entropy', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False), 'classifier__criterion': 'entropy', 'classifier__n_estimators': 50} \n",
            "Melhor score = 0.22227663843825463\n",
            "Hamming Loss = 0.09197324414715718\n",
            "Acurácia = 0.532608695652174\n",
            "Precisão = 0.8920245398773006\n",
            "Revocação = 0.5596612779060816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTzRAeCAQoTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "98427af4-9dd9-479f-93de-6aacfb410ab5"
      },
      "source": [
        "# MLkNN\n",
        "print(\"MLkNN simples:\")\n",
        "MLkNN_simples(x_train_scene, y_train_scene, x_test_scene, y_test_scene)\n",
        "\n",
        "print(\"\\nMLkNN com Grid Search:\")\n",
        "MLkNN_Hamming_Loss_scene , MLkNN_Acuracia_scene, MLkNN_Precisão_scene , MLkNN_Revocação_scene = MLkNN_GS(x_train_scene, y_train_scene, x_test_scene, y_test_scene)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLkNN simples:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-ce2fdc7661b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# MLkNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MLkNN simples:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mMLkNN_simples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_scene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_scene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_scene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_scene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMLkNN com Grid Search:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-705ac038f8eb>\u001b[0m in \u001b[0;36mMLkNN_simples\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mMLkNN_simples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLkNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hamming Loss =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhamming_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLxoEE1eQqB6",
        "outputId": "adbb9306-6f10-4f11-cc92-17120b96b201"
      },
      "source": [
        "# LP\n",
        "print(\"LP simples:\")\n",
        "LP_simples(x_train_scene, y_train_scene, x_test_scene, y_test_scene)\n",
        "\n",
        "print(\"\\nLP com Grid Search:\")\n",
        "LP_Hamming_Loss_scene , LP_Acuracia_scene, LP_Precisão_scene , LP_Revocação_scene = LP_GS(x_train_scene, y_train_scene, x_test_scene, y_test_scene)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LP simples:\n",
            "Hamming Loss = 0.08876811594202899\n",
            "Acurácia = 0.5568561872909699\n",
            "Precisão = 0.9046454767726161\n",
            "Revocação = 0.5696689761354888\n",
            "\n",
            "LP com Grid Search:\n",
            "Melhores parâmetros = {'classifier': GaussianNB(priors=None, var_smoothing=1e-08), 'classifier__var_smoothing': 1e-08} \n",
            "Melhor score = 0.3669047376118083\n",
            "Hamming Loss = 0.13447603121516166\n",
            "Acurácia = 0.5560200668896321\n",
            "Precisão = 0.6272865853658537\n",
            "Revocação = 0.6335642802155504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.13447603121516166,\n",
              " 0.5560200668896321,\n",
              " 0.6272865853658537,\n",
              " 0.6335642802155504)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmdB8cZXuAW9",
        "outputId": "bda232ef-bcb0-426e-c5ba-fa3d43836483"
      },
      "source": [
        "BR_Hamming_Loss_birds"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.048069089131497475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "DfiQyQC5uBKV",
        "outputId": "6c5e2ac6-bda6-49c4-e3e5-c3b676a690c7"
      },
      "source": [
        "labels = ['Hamming_Loss', 'Acurácia', 'Precisão', 'Revocação']\n",
        "Emotions_br = [BR_Hamming_Loss_Emotions, BR_Acuracia_Emotions, BR_Precisão_Emotions, BR_Revocação_Emotions]\n",
        "Emotions_MLkNN = [MLkNN_Hamming_Loss_Emotions, MLkNN_Acuracia_Emotions, MLkNN_Precisão_Emotions, MLkNN_Revocação_Emotions]\n",
        "Emotions_LP = [LP_Hamming_Loss_Emotions, LP_Acuracia_Emotions, LP_Precisão_Emotions, LP_Revocação_Emotions]\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width = 0.60  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/3, Emotions_br, width/3, label='Br')\n",
        "rects2 = ax.bar(x, Emotions_MLkNN, width/3, label='MLKNN')\n",
        "rects3 = ax.bar(x + width/3, Emotions_LP, width/3, label='LP')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel(' ')\n",
        "ax.set_title(' ')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 3, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}